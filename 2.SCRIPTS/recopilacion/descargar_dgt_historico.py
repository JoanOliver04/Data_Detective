# -*- coding: utf-8 -*-
"""
==============================================================================
DATA DETECTIVE - VALENCIA
Fase 2.4: InvestigaciÃ³n de Datos de TrÃ¡fico DGT (DATEX II)
==============================================================================

DescripciÃ³n:
    Este script investiga la disponibilidad de datos de trÃ¡fico de la DGT.
    
    CONCLUSIÃ“N IMPORTANTE:
    La DGT NO ofrece datos histÃ³ricos pÃºblicos vÃ­a API.
    El endpoint DATEX II proporciona Ãºnicamente datos en TIEMPO REAL.
    
    Los datos histÃ³ricos deben construirse mediante acumulaciÃ³n en la Fase 3
    (recopilaciÃ³n de datos dinÃ¡micos).

Endpoints investigados:
    - https://infocar.dgt.es/datex2/dgt/TrafficData (tiempo real)
    - https://infocar.dgt.es/datex2/dgt/SituationPublication (incidencias)
    
Formato: XML DATEX II (estÃ¡ndar europeo de intercambio de datos de trÃ¡fico)

Uso:
    python descargar_dgt_historico.py
    
    El script:
    1. Realiza una peticiÃ³n al endpoint de la DGT
    2. Analiza la estructura del XML
    3. Guarda una muestra del estado actual
    4. Documenta las limitaciones encontradas

Autor: Joan
Fecha: 2026
Proyecto: Data Detective Valencia
"""

import requests
import logging
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any
from bs4 import BeautifulSoup
import sys

# ==============================================================================
# CONFIGURACIÃ“N
# ==============================================================================

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
OUTPUT_DIR = PROJECT_ROOT / "1.DATOS_EN_CRUDO" / "estaticos" / "trafico"
LOG_DIR = PROJECT_ROOT / "logs"

# Endpoints DGT DATEX II
DGT_ENDPOINTS = {
    "traffic_data": "https://infocar.dgt.es/datex2/dgt/TrafficData",
    "incidencias": "https://infocar.dgt.es/datex2/dgt/SituationPublication/all/content.xml",
    "camaras": "https://infocar.dgt.es/datex2/dgt/CCTVSiteTablePublication/all/content.xml",
}

# Headers para las peticiones
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) DataDetective/1.0",
    "Accept": "application/xml, text/xml, */*",
}

# Timeout para las peticiones
REQUEST_TIMEOUT = 30


# ==============================================================================
# CONFIGURACIÃ“N DE LOGGING
# ==============================================================================

def setup_logging() -> logging.Logger:
    """Configura el sistema de logging."""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    
    log_file = LOG_DIR / "dgt_historico.log"
    log_format = "%(asctime)s - %(levelname)s - %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"
    
    logger = logging.getLogger("DGT_Historico")
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()
    
    file_handler = logging.FileHandler(log_file, encoding="utf-8", mode="a")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger


# ==============================================================================
# FUNCIONES DE INVESTIGACIÃ“N
# ==============================================================================

def fetch_dgt_endpoint(url: str, logger: logging.Logger) -> Optional[str]:
    """
    Realiza una peticiÃ³n GET a un endpoint de la DGT.
    
    Args:
        url: URL del endpoint
        logger: Logger
    
    Returns:
        Contenido XML como string o None si hay error
    """
    try:
        logger.info(f"Consultando: {url}")
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        
        if response.status_code == 200:
            logger.info(f"  âœ“ Respuesta OK ({len(response.content)} bytes)")
            return response.text
        else:
            logger.warning(f"  âœ— HTTP {response.status_code}")
            return None
            
    except requests.exceptions.Timeout:
        logger.error(f"  âœ— Timeout despuÃ©s de {REQUEST_TIMEOUT}s")
        return None
    except requests.exceptions.RequestException as e:
        logger.error(f"  âœ— Error de conexiÃ³n: {str(e)}")
        return None


def analyze_xml_structure(xml_content: str, logger: logging.Logger) -> Dict[str, Any]:
    """
    Analiza la estructura del XML DATEX II.
    
    Args:
        xml_content: Contenido XML
        logger: Logger
    
    Returns:
        Diccionario con informaciÃ³n sobre la estructura
    """
    analysis = {
        "tiene_datos": False,
        "tipo_publicacion": None,
        "fecha_publicacion": None,
        "num_elementos": 0,
        "elementos_ejemplo": [],
        "namespaces": [],
        "es_tiempo_real": True,  # Por defecto, asumimos tiempo real
        "tiene_historicos": False,
    }
    
    try:
        soup = BeautifulSoup(xml_content, "lxml-xml")
        
        # Buscar elemento raÃ­z
        root = soup.find()
        if root:
            analysis["tipo_publicacion"] = root.name
            logger.debug(f"Tipo de publicaciÃ³n: {root.name}")
        
        # Buscar fecha de publicaciÃ³n
        pub_time = soup.find("publicationTime")
        if pub_time:
            analysis["fecha_publicacion"] = pub_time.text
            logger.info(f"  Fecha de publicaciÃ³n: {pub_time.text}")
        
        # Contar elementos de datos
        # En DATEX II, los datos suelen estar en elementos como:
        # - siteMeasurements (datos de trÃ¡fico)
        # - situation (incidencias)
        # - camera (cÃ¡maras)
        
        data_elements = []
        
        # Buscar mediciones de trÃ¡fico
        measurements = soup.find_all("siteMeasurements")
        if measurements:
            data_elements.extend(measurements)
            logger.info(f"  Mediciones de trÃ¡fico encontradas: {len(measurements)}")
        
        # Buscar incidencias
        situations = soup.find_all("situation")
        if situations:
            data_elements.extend(situations)
            logger.info(f"  Incidencias encontradas: {len(situations)}")
        
        # Buscar cÃ¡maras
        cameras = soup.find_all("cctvcamera") or soup.find_all("camera")
        if cameras:
            data_elements.extend(cameras)
            logger.info(f"  CÃ¡maras encontradas: {len(cameras)}")
        
        analysis["num_elementos"] = len(data_elements)
        analysis["tiene_datos"] = len(data_elements) > 0
        
        # Obtener ejemplos de elementos (primeros 3)
        for elem in data_elements[:3]:
            # Extraer ID si existe
            elem_id = elem.get("id") or elem.find("id")
            if elem_id:
                if hasattr(elem_id, "text"):
                    analysis["elementos_ejemplo"].append(elem_id.text)
                else:
                    analysis["elementos_ejemplo"].append(str(elem_id))
        
        # Verificar si hay parÃ¡metros de fecha/histÃ³ricos
        # (Normalmente no los hay en endpoints de tiempo real)
        historic_indicators = soup.find_all(["historicData", "archiveData", "dateRange"])
        if historic_indicators:
            analysis["tiene_historicos"] = True
            analysis["es_tiempo_real"] = False
            logger.info("  âš  Se encontraron indicadores de datos histÃ³ricos")
        else:
            logger.info("  â„¹ Solo datos en tiempo real (sin histÃ³ricos)")
        
    except Exception as e:
        logger.error(f"Error analizando XML: {str(e)}")
    
    return analysis


def save_sample(xml_content: str, filename: str, logger: logging.Logger) -> Optional[Path]:
    """
    Guarda una muestra del XML actual.
    
    Args:
        xml_content: Contenido XML
        filename: Nombre del archivo
        logger: Logger
    
    Returns:
        Ruta al archivo guardado o None
    """
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # AÃ±adir timestamp al nombre
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = OUTPUT_DIR / f"{filename}_{timestamp}.xml"
    
    try:
        # Guardar solo los primeros 50KB como muestra
        sample = xml_content[:50000]
        if len(xml_content) > 50000:
            sample += "\n\n<!-- ... contenido truncado (muestra de 50KB) ... -->"
        
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(sample)
        
        logger.info(f"  âœ“ Muestra guardada: {output_file.name}")
        return output_file
        
    except Exception as e:
        logger.error(f"Error guardando muestra: {str(e)}")
        return None


def generate_readme(analysis_results: Dict[str, Dict], logger: logging.Logger) -> Path:
    """
    Genera el archivo README documentando las limitaciones.
    
    Args:
        analysis_results: Resultados del anÃ¡lisis de cada endpoint
        logger: Logger
    
    Returns:
        Ruta al archivo README generado
    """
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    readme_path = OUTPUT_DIR / "README_dgt_historico.md"
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    content = f"""# Datos de TrÃ¡fico DGT - DocumentaciÃ³n

## InvestigaciÃ³n realizada: {timestamp}

---

## âš ï¸ CONCLUSIÃ“N PRINCIPAL

**La DGT NO ofrece datos histÃ³ricos de trÃ¡fico pÃºblicos vÃ­a API.**

Los endpoints DATEX II proporcionan Ãºnicamente **datos en tiempo real**.

---

## ğŸ“¡ Endpoints Investigados

### 1. TrafficData (Datos de TrÃ¡fico)
- **URL**: `https://infocar.dgt.es/datex2/dgt/TrafficData`
- **Tipo**: Tiempo real
- **Formato**: XML DATEX II
- **Contenido**: Mediciones de intensidad, velocidad y ocupaciÃ³n de la red estatal
- **ActualizaciÃ³n**: Cada pocos minutos
- **HistÃ³ricos disponibles**: âŒ NO

### 2. SituationPublication (Incidencias)
- **URL**: `https://infocar.dgt.es/datex2/dgt/SituationPublication/all/content.xml`
- **Tipo**: Tiempo real
- **Formato**: XML DATEX II
- **Contenido**: Incidencias activas (obras, accidentes, retenciones)
- **HistÃ³ricos disponibles**: âŒ NO

### 3. CCTVSiteTablePublication (CÃ¡maras)
- **URL**: `https://infocar.dgt.es/datex2/dgt/CCTVSiteTablePublication/all/content.xml`
- **Tipo**: Tiempo real
- **Formato**: XML DATEX II
- **Contenido**: UbicaciÃ³n y estado de cÃ¡maras de trÃ¡fico
- **HistÃ³ricos disponibles**: âŒ NO

---

## ğŸ” Resultados del AnÃ¡lisis

"""
    
    for endpoint_name, analysis in analysis_results.items():
        content += f"""### {endpoint_name}
- Datos encontrados: {"âœ“ SÃ­" if analysis.get("tiene_datos") else "âœ— No"}
- Fecha de publicaciÃ³n: {analysis.get("fecha_publicacion", "N/A")}
- NÃºmero de elementos: {analysis.get("num_elementos", 0)}
- Es tiempo real: {"âœ“ SÃ­" if analysis.get("es_tiempo_real") else "No"}
- Tiene histÃ³ricos: {"âœ“ SÃ­" if analysis.get("tiene_historicos") else "âœ— No"}

"""
    
    content += """---

## ğŸ“‹ Formato DATEX II

DATEX II es el estÃ¡ndar europeo para intercambio de datos de trÃ¡fico:

- **EspecificaciÃ³n**: [docs.datex2.eu](https://docs.datex2.eu/)
- **Versiones**: La DGT usa v1.0 y v3.x segÃºn el endpoint
- **Estructura**: XML con namespaces especÃ­ficos
- **Elementos principales**:
  - `siteMeasurements`: Mediciones de puntos de aforo
  - `situation`: Incidencias de trÃ¡fico
  - `cctvcamera`: Datos de cÃ¡maras

---

## ğŸš§ Limitaciones Identificadas

1. **Sin API de histÃ³ricos**: No existe endpoint para consultar datos pasados
2. **Sin parÃ¡metros de fecha**: Los endpoints no aceptan rangos temporales
3. **Solo red estatal**: Excluye CataluÃ±a y PaÃ­s Vasco
4. **Cobertura Valencia**: Solo carreteras estatales (A-3, A-7, V-30, etc.)

---

## âœ… Estrategia para Data Detective

### Fase 2 (Actual)
- âœ“ Documentar la limitaciÃ³n (este archivo)
- âœ“ Guardar muestra del formato XML actual
- âœ“ No inventar datos histÃ³ricos

### Fase 3 (Datos DinÃ¡micos)
- Implementar script de captura periÃ³dica
- Programar con Task Scheduler (cada 5-10 minutos)
- Acumular datos en: `1.DATOS_EN_CRUDO/dinamicos/trafico/`
- Construir histÃ³rico propio por acumulaciÃ³n

### Formato de AcumulaciÃ³n Propuesto
```
fecha,hora,punto_medida,intensidad,velocidad,ocupacion
2026-02-06,14:30:00,PM_V30_KM5,1250,78,45
```

---

## ğŸ“š Referencias

- [Portal DATEX II DGT](https://infocar.dgt.es/datex2/)
- [GuÃ­a de UtilizaciÃ³n DATEX II](https://infocar.dgt.es/datex2/informacion_adicional/Guia%20de%20Utilizacion%20de%20DATEX%20II.pdf)
- [NAP - Punto de Acceso Nacional](https://nap.dgt.es/)
- [EspecificaciÃ³n DATEX II](https://docs.datex2.eu/)

---

## ğŸ“ Archivos en este directorio

- `README_dgt_historico.md` - Este archivo de documentaciÃ³n
- `muestra_traffic_*.xml` - Muestra del XML de trÃ¡fico en tiempo real
- `muestra_incidencias_*.xml` - Muestra del XML de incidencias (si disponible)

---

*Generado automÃ¡ticamente por Data Detective - Fase 2.4*
"""
    
    with open(readme_path, "w", encoding="utf-8") as f:
        f.write(content)
    
    logger.info(f"âœ“ README generado: {readme_path.name}")
    return readme_path


# ==============================================================================
# FUNCIÃ“N PRINCIPAL
# ==============================================================================

def main():
    """FunciÃ³n principal que investiga los datos de la DGT."""
    
    logger = setup_logging()
    logger.info("=" * 70)
    logger.info("INICIO: InvestigaciÃ³n de datos de trÃ¡fico DGT (DATEX II)")
    logger.info("=" * 70)
    
    # Crear directorio de salida
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    analysis_results = {}
    
    # Investigar cada endpoint
    for endpoint_name, url in DGT_ENDPOINTS.items():
        logger.info(f"\n{'â”€' * 50}")
        logger.info(f"Investigando: {endpoint_name}")
        logger.info(f"{'â”€' * 50}")
        
        # Obtener datos
        xml_content = fetch_dgt_endpoint(url, logger)
        
        if xml_content:
            # Analizar estructura
            analysis = analyze_xml_structure(xml_content, logger)
            analysis_results[endpoint_name] = analysis
            
            # Guardar muestra
            if analysis["tiene_datos"]:
                save_sample(xml_content, f"muestra_{endpoint_name}", logger)
        else:
            analysis_results[endpoint_name] = {
                "tiene_datos": False,
                "error": "No se pudo obtener respuesta"
            }
    
    # Generar documentaciÃ³n
    logger.info(f"\n{'â”€' * 50}")
    logger.info("GENERANDO DOCUMENTACIÃ“N")
    logger.info(f"{'â”€' * 50}")
    
    readme_path = generate_readme(analysis_results, logger)
    
    # Resumen final
    logger.info("")
    logger.info("=" * 70)
    logger.info("INVESTIGACIÃ“N COMPLETADA")
    logger.info("=" * 70)
    logger.info("")
    logger.info("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    logger.info("â•‘  CONCLUSIÃ“N: La DGT NO ofrece datos histÃ³ricos pÃºblicos             â•‘")
    logger.info("â•‘                                                                      â•‘")
    logger.info("â•‘  Los datos histÃ³ricos de trÃ¡fico se construirÃ¡n por ACUMULACIÃ“N     â•‘")
    logger.info("â•‘  en la Fase 3 (Datos DinÃ¡micos) mediante captura periÃ³dica.         â•‘")
    logger.info("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    logger.info("")
    logger.info(f"DocumentaciÃ³n generada: {readme_path}")
    logger.info(f"Muestras guardadas en: {OUTPUT_DIR}")
    logger.info("")


# ==============================================================================
# PUNTO DE ENTRADA
# ==============================================================================

if __name__ == "__main__":
    main()
