# -*- coding: utf-8 -*-
"""
==============================================================================
DATA DETECTIVE - VALENCIA
Fase 2.5: Verificaci√≥n de Datos Est√°ticos
==============================================================================

Descripci√≥n:
    Este script verifica todos los datos est√°ticos obtenidos durante la Fase 2.
    Genera un informe completo con estad√≠sticas y limitaciones documentadas.

Fuentes verificadas:
    - GVA (Contaminaci√≥n atmosf√©rica)
    - EEA (European Environment Agency)
    - AEMET (Meteorolog√≠a)
    - DGT (Tr√°fico)

Uso:
    python verificar_datos_estaticos.py
    
Salida:
    - logs/informe_fase2.md (informe completo)
    - Resumen en consola

Autor: Joan
Fecha: 2026
Proyecto: Data Detective Valencia
"""

import pandas as pd
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
import sys

# ==============================================================================
# CONFIGURACI√ìN
# ==============================================================================

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
DATOS_ESTATICOS_DIR = PROJECT_ROOT / "1.DATOS_EN_CRUDO" / "estaticos"
LOG_DIR = PROJECT_ROOT / "logs"

# Estructura esperada de carpetas
FUENTES_ESPERADAS = {
    "contaminacion": {
        "nombre": "GVA - Calidad del Aire",
        "descripcion": "Datos hist√≥ricos de contaminaci√≥n de la Generalitat Valenciana",
        "variables": ["NO2", "SO2", "O3", "PM10", "PM2.5", "CO"],
    },
    "eea": {
        "nombre": "EEA - European Environment Agency",
        "descripcion": "Datos europeos de calidad del aire",
        "variables": ["NO2", "O3", "PM10", "PM2.5"],
    },
    "meteorologia": {
        "nombre": "AEMET - Meteorolog√≠a",
        "descripcion": "Datos meteorol√≥gicos hist√≥ricos",
        "variables": ["precipitacion", "temperatura", "humedad", "viento"],
    },
    "trafico": {
        "nombre": "DGT - Tr√°fico",
        "descripcion": "Datos de tr√°fico de la red estatal",
        "variables": ["intensidad", "velocidad", "incidencias"],
    },
}


# ==============================================================================
# CONFIGURACI√ìN DE LOGGING
# ==============================================================================

def setup_logging() -> logging.Logger:
    """Configura el sistema de logging."""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    
    log_file = LOG_DIR / "verificacion_fase2.log"
    log_format = "%(asctime)s - %(levelname)s - %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"
    
    logger = logging.getLogger("Verificacion_Fase2")
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()
    
    file_handler = logging.FileHandler(log_file, encoding="utf-8", mode="a")
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger


# ==============================================================================
# FUNCIONES DE AN√ÅLISIS
# ==============================================================================

def analizar_csv(file_path: Path, logger: logging.Logger) -> Dict[str, Any]:
    """
    Analiza un archivo CSV y extrae estad√≠sticas.
    
    Args:
        file_path: Ruta al archivo CSV
        logger: Logger
    
    Returns:
        Diccionario con estad√≠sticas del archivo
    """
    stats = {
        "tipo": "CSV",
        "tama√±o_bytes": file_path.stat().st_size,
        "registros": 0,
        "columnas": [],
        "fecha_min": None,
        "fecha_max": None,
        "variables": [],
        "estaciones": [],
        "error": None,
    }
    
    try:
        # Leer solo las primeras filas para obtener estructura
        df_sample = pd.read_csv(file_path, nrows=5)
        stats["columnas"] = list(df_sample.columns)
        
        # Contar registros totales (sin cargar todo en memoria)
        with open(file_path, 'r', encoding='utf-8') as f:
            stats["registros"] = sum(1 for _ in f) - 1  # -1 por header
        
        # Si tiene columnas esperadas, extraer m√°s info
        if "fecha" in df_sample.columns:
            # Leer solo columna fecha para obtener rango
            df_fechas = pd.read_csv(file_path, usecols=["fecha"], parse_dates=["fecha"])
            stats["fecha_min"] = df_fechas["fecha"].min().strftime("%Y-%m-%d")
            stats["fecha_max"] = df_fechas["fecha"].max().strftime("%Y-%m-%d")
        
        if "variable" in df_sample.columns:
            df_vars = pd.read_csv(file_path, usecols=["variable"])
            stats["variables"] = df_vars["variable"].unique().tolist()
        
        if "estacion" in df_sample.columns:
            df_est = pd.read_csv(file_path, usecols=["estacion"])
            stats["estaciones"] = df_est["estacion"].unique().tolist()
            
    except Exception as e:
        stats["error"] = str(e)
        logger.warning(f"  Error analizando {file_path.name}: {e}")
    
    return stats


def analizar_parquet(file_path: Path, logger: logging.Logger) -> Dict[str, Any]:
    """
    Analiza un archivo Parquet y extrae estad√≠sticas.
    
    Args:
        file_path: Ruta al archivo Parquet
        logger: Logger
    
    Returns:
        Diccionario con estad√≠sticas del archivo
    """
    stats = {
        "tipo": "Parquet",
        "tama√±o_bytes": file_path.stat().st_size,
        "registros": 0,
        "columnas": [],
        "fecha_min": None,
        "fecha_max": None,
        "error": None,
    }
    
    try:
        df = pd.read_parquet(file_path)
        stats["registros"] = len(df)
        stats["columnas"] = list(df.columns)
        
        # Buscar columnas de fecha
        for col in ["Start", "fecha", "date", "datetime"]:
            if col in df.columns:
                stats["fecha_min"] = df[col].min()
                stats["fecha_max"] = df[col].max()
                if hasattr(stats["fecha_min"], "strftime"):
                    stats["fecha_min"] = stats["fecha_min"].strftime("%Y-%m-%d")
                    stats["fecha_max"] = stats["fecha_max"].strftime("%Y-%m-%d")
                break
                
    except Exception as e:
        stats["error"] = str(e)
        logger.warning(f"  Error analizando {file_path.name}: {e}")
    
    return stats


def analizar_xml(file_path: Path, logger: logging.Logger) -> Dict[str, Any]:
    """
    Analiza un archivo XML (muestra de DGT).
    
    Args:
        file_path: Ruta al archivo XML
        logger: Logger
    
    Returns:
        Diccionario con estad√≠sticas del archivo
    """
    stats = {
        "tipo": "XML",
        "tama√±o_bytes": file_path.stat().st_size,
        "es_muestra": "muestra" in file_path.name.lower(),
        "error": None,
    }
    
    return stats


def analizar_markdown(file_path: Path, logger: logging.Logger) -> Dict[str, Any]:
    """
    Analiza un archivo Markdown (documentaci√≥n).
    
    Args:
        file_path: Ruta al archivo Markdown
        logger: Logger
    
    Returns:
        Diccionario con estad√≠sticas del archivo
    """
    stats = {
        "tipo": "Documentaci√≥n",
        "tama√±o_bytes": file_path.stat().st_size,
        "es_readme": "readme" in file_path.name.lower(),
        "error": None,
    }
    
    return stats


def analizar_directorio(dir_path: Path, logger: logging.Logger) -> Dict[str, Any]:
    """
    Analiza un directorio de fuente de datos.
    
    Args:
        dir_path: Ruta al directorio
        logger: Logger
    
    Returns:
        Diccionario con an√°lisis completo del directorio
    """
    resultado = {
        "existe": dir_path.exists(),
        "archivos": [],
        "total_archivos": 0,
        "total_registros": 0,
        "total_bytes": 0,
        "fecha_min_global": None,
        "fecha_max_global": None,
        "tiene_datos": False,
        "tiene_documentacion": False,
        "archivos_vacios": [],
        "errores": [],
    }
    
    if not dir_path.exists():
        return resultado
    
    # Buscar todos los archivos (incluyendo subdirectorios)
    all_files = list(dir_path.rglob("*"))
    archivos = [f for f in all_files if f.is_file()]
    
    resultado["total_archivos"] = len(archivos)
    
    fechas_min = []
    fechas_max = []
    
    for archivo in archivos:
        file_info = {
            "nombre": archivo.name,
            "ruta_relativa": str(archivo.relative_to(dir_path)),
            "extension": archivo.suffix.lower(),
        }
        
        # Detectar archivos vac√≠os
        if archivo.stat().st_size == 0:
            resultado["archivos_vacios"].append(archivo.name)
            file_info["vacio"] = True
            resultado["archivos"].append(file_info)
            continue
        
        # Analizar seg√∫n tipo
        if archivo.suffix.lower() == ".csv":
            stats = analizar_csv(archivo, logger)
            file_info.update(stats)
            resultado["tiene_datos"] = True
            resultado["total_registros"] += stats.get("registros", 0)
            
            if stats.get("fecha_min"):
                fechas_min.append(stats["fecha_min"])
            if stats.get("fecha_max"):
                fechas_max.append(stats["fecha_max"])
                
        elif archivo.suffix.lower() == ".parquet":
            stats = analizar_parquet(archivo, logger)
            file_info.update(stats)
            resultado["tiene_datos"] = True
            resultado["total_registros"] += stats.get("registros", 0)
            
            if stats.get("fecha_min"):
                fechas_min.append(stats["fecha_min"])
            if stats.get("fecha_max"):
                fechas_max.append(stats["fecha_max"])
                
        elif archivo.suffix.lower() == ".xml":
            stats = analizar_xml(archivo, logger)
            file_info.update(stats)
            resultado["tiene_datos"] = True
            
        elif archivo.suffix.lower() == ".md":
            stats = analizar_markdown(archivo, logger)
            file_info.update(stats)
            resultado["tiene_documentacion"] = True
            
        else:
            file_info["tipo"] = "Otro"
        
        file_info["tama√±o_bytes"] = archivo.stat().st_size
        resultado["total_bytes"] += archivo.stat().st_size
        resultado["archivos"].append(file_info)
        
        if file_info.get("error"):
            resultado["errores"].append(f"{archivo.name}: {file_info['error']}")
    
    # Calcular rango temporal global
    if fechas_min:
        resultado["fecha_min_global"] = min(fechas_min)
    if fechas_max:
        resultado["fecha_max_global"] = max(fechas_max)
    
    return resultado


def formatear_bytes(bytes_val: int) -> str:
    """Formatea bytes a unidad legible."""
    if bytes_val < 1024:
        return f"{bytes_val} B"
    elif bytes_val < 1024 * 1024:
        return f"{bytes_val / 1024:.1f} KB"
    elif bytes_val < 1024 * 1024 * 1024:
        return f"{bytes_val / (1024 * 1024):.1f} MB"
    else:
        return f"{bytes_val / (1024 * 1024 * 1024):.2f} GB"


# ==============================================================================
# GENERACI√ìN DE INFORME
# ==============================================================================

def generar_informe(resultados: Dict[str, Dict], logger: logging.Logger) -> Path:
    """
    Genera el informe de verificaci√≥n en formato Markdown.
    
    Args:
        resultados: Diccionario con resultados por fuente
        logger: Logger
    
    Returns:
        Ruta al archivo de informe generado
    """
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    informe_path = LOG_DIR / "informe_fase2.md"
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Calcular totales
    total_archivos = sum(r.get("total_archivos", 0) for r in resultados.values())
    total_registros = sum(r.get("total_registros", 0) for r in resultados.values())
    total_bytes = sum(r.get("total_bytes", 0) for r in resultados.values())
    
    content = f"""# üìä Informe de Verificaci√≥n - Fase 2: Datos Est√°ticos

**Proyecto**: Data Detective Valencia  
**Fecha de verificaci√≥n**: {timestamp}  
**Directorio analizado**: `1.DATOS_EN_CRUDO/estaticos/`

---

## üìà Resumen Ejecutivo

| M√©trica | Valor |
|---------|-------|
| **Fuentes verificadas** | {len(resultados)} |
| **Total archivos** | {total_archivos} |
| **Total registros** | {total_registros:,} |
| **Tama√±o total** | {formatear_bytes(total_bytes)} |

### Estado por Fuente

| Fuente | Datos | Documentaci√≥n | Registros | Periodo |
|--------|:-----:|:-------------:|----------:|---------|
"""
    
    for fuente, resultado in resultados.items():
        info = FUENTES_ESPERADAS.get(fuente, {})
        nombre = info.get("nombre", fuente.upper())
        tiene_datos = "‚úÖ" if resultado.get("tiene_datos") else "‚ùå"
        tiene_doc = "‚úÖ" if resultado.get("tiene_documentacion") else "‚ûñ"
        registros = f"{resultado.get('total_registros', 0):,}"
        
        if resultado.get("fecha_min_global") and resultado.get("fecha_max_global"):
            periodo = f"{resultado['fecha_min_global']} ‚Üí {resultado['fecha_max_global']}"
        else:
            periodo = "N/A"
        
        content += f"| {nombre} | {tiene_datos} | {tiene_doc} | {registros} | {periodo} |\n"
    
    content += """
---

## üìÅ Detalle por Fuente

"""
    
    for fuente, resultado in resultados.items():
        info = FUENTES_ESPERADAS.get(fuente, {})
        nombre = info.get("nombre", fuente.upper())
        descripcion = info.get("descripcion", "")
        
        content += f"""### {nombre}

**Descripci√≥n**: {descripcion}  
**Directorio**: `1.DATOS_EN_CRUDO/estaticos/{fuente}/`

"""
        
        if not resultado.get("existe"):
            content += "> ‚ö†Ô∏è **Directorio no encontrado**\n\n"
            continue
        
        if resultado.get("total_archivos", 0) == 0:
            content += "> ‚ÑπÔ∏è **Directorio vac√≠o**\n\n"
            continue
        
        # Estad√≠sticas
        content += f"""**Estad√≠sticas**:
- Archivos: {resultado.get('total_archivos', 0)}
- Registros totales: {resultado.get('total_registros', 0):,}
- Tama√±o: {formatear_bytes(resultado.get('total_bytes', 0))}
"""
        
        if resultado.get("fecha_min_global"):
            content += f"- Periodo: {resultado['fecha_min_global']} ‚Üí {resultado['fecha_max_global']}\n"
        
        content += "\n**Archivos**:\n\n"
        content += "| Archivo | Tipo | Registros | Tama√±o |\n"
        content += "|---------|------|----------:|-------:|\n"
        
        for archivo in resultado.get("archivos", []):
            nombre_archivo = archivo.get("ruta_relativa", archivo.get("nombre", "?"))
            tipo = archivo.get("tipo", "?")
            registros = archivo.get("registros", "-")
            if isinstance(registros, int):
                registros = f"{registros:,}"
            tama√±o = formatear_bytes(archivo.get("tama√±o_bytes", 0))
            
            content += f"| `{nombre_archivo}` | {tipo} | {registros} | {tama√±o} |\n"
        
        # Archivos vac√≠os
        if resultado.get("archivos_vacios"):
            content += f"\n> ‚ö†Ô∏è **Archivos vac√≠os**: {', '.join(resultado['archivos_vacios'])}\n"
        
        # Errores
        if resultado.get("errores"):
            content += "\n> ‚ùå **Errores encontrados**:\n"
            for error in resultado["errores"]:
                content += f"> - {error}\n"
        
        content += "\n"
    
    # Secci√≥n de limitaciones
    content += """---

## ‚ö†Ô∏è Limitaciones Documentadas

### DGT - Tr√°fico
- **Sin datos hist√≥ricos p√∫blicos** v√≠a API
- Los endpoints DATEX II solo ofrecen datos en tiempo real
- Los hist√≥ricos se construir√°n por acumulaci√≥n en Fase 3

### AEMET - Meteorolog√≠a
- API con **rate limiting** estricto
- No todos los datos hist√≥ricos disponibles v√≠a API
- Datos anteriores a cierta fecha requieren solicitud directa a AEMET

### GVA - Contaminaci√≥n
- Datos descargados **manualmente** desde portal web
- No existe API REST p√∫blica para descarga masiva

### EEA - Datos Europeos
- Archivos **muy grandes** (requieren procesamiento con chunks)
- Descarga manual desde portal

---

## ‚úÖ Conclusiones

"""
    
    # Determinar conclusiones autom√°ticas
    fuentes_con_datos = sum(1 for r in resultados.values() if r.get("tiene_datos"))
    fuentes_documentadas = sum(1 for r in resultados.values() if r.get("tiene_documentacion"))
    
    if fuentes_con_datos >= 3:
        content += "‚úÖ **Fase 2 completada satisfactoriamente**\n\n"
    else:
        content += "‚ö†Ô∏è **Fase 2 parcialmente completada**\n\n"
    
    content += f"""- {fuentes_con_datos}/4 fuentes con datos recopilados
- {fuentes_documentadas}/4 fuentes con documentaci√≥n
- Total de {total_registros:,} registros disponibles para an√°lisis
- Tama√±o total del dataset: {formatear_bytes(total_bytes)}

### Pr√≥ximos pasos (Fase 3)
1. Implementar scripts de captura de datos din√°micos
2. Configurar Task Scheduler para automatizaci√≥n
3. Comenzar acumulaci√≥n de hist√≥ricos de tr√°fico DGT

---

*Informe generado autom√°ticamente por Data Detective*  
*Verificaci√≥n de Fase 2 - {timestamp}*
"""
    
    with open(informe_path, "w", encoding="utf-8") as f:
        f.write(content)
    
    logger.info(f"‚úì Informe generado: {informe_path}")
    return informe_path


# ==============================================================================
# FUNCI√ìN PRINCIPAL
# ==============================================================================

def main():
    """Funci√≥n principal de verificaci√≥n."""
    
    logger = setup_logging()
    logger.info("=" * 70)
    logger.info("INICIO: Verificaci√≥n de Datos Est√°ticos (Fase 2)")
    logger.info("=" * 70)
    
    # Verificar que existe el directorio base
    if not DATOS_ESTATICOS_DIR.exists():
        logger.error(f"Directorio no encontrado: {DATOS_ESTATICOS_DIR}")
        logger.info("Aseg√∫rate de haber ejecutado los scripts de las fases 2.1 a 2.4")
        return
    
    logger.info(f"Directorio base: {DATOS_ESTATICOS_DIR}")
    
    # Analizar cada fuente
    resultados = {}
    
    for fuente in FUENTES_ESPERADAS.keys():
        logger.info(f"\n{'‚îÄ' * 50}")
        logger.info(f"Verificando: {FUENTES_ESPERADAS[fuente]['nombre']}")
        logger.info(f"{'‚îÄ' * 50}")
        
        fuente_dir = DATOS_ESTATICOS_DIR / fuente
        resultado = analizar_directorio(fuente_dir, logger)
        resultados[fuente] = resultado
        
        if resultado["existe"]:
            logger.info(f"  Archivos encontrados: {resultado['total_archivos']}")
            logger.info(f"  Registros totales: {resultado['total_registros']:,}")
            logger.info(f"  Tama√±o: {formatear_bytes(resultado['total_bytes'])}")
            
            if resultado["fecha_min_global"]:
                logger.info(f"  Periodo: {resultado['fecha_min_global']} ‚Üí {resultado['fecha_max_global']}")
            
            if resultado["archivos_vacios"]:
                logger.warning(f"  ‚ö† Archivos vac√≠os: {len(resultado['archivos_vacios'])}")
        else:
            logger.warning(f"  ‚úó Directorio no encontrado")
    
    # Generar informe
    logger.info(f"\n{'‚îÄ' * 50}")
    logger.info("GENERANDO INFORME")
    logger.info(f"{'‚îÄ' * 50}")
    
    informe_path = generar_informe(resultados, logger)
    
    # Resumen final
    total_registros = sum(r.get("total_registros", 0) for r in resultados.values())
    total_bytes = sum(r.get("total_bytes", 0) for r in resultados.values())
    fuentes_con_datos = sum(1 for r in resultados.values() if r.get("tiene_datos"))
    
    logger.info("")
    logger.info("=" * 70)
    logger.info("VERIFICACI√ìN COMPLETADA")
    logger.info("=" * 70)
    logger.info("")
    logger.info(f"  Fuentes con datos: {fuentes_con_datos}/4")
    logger.info(f"  Total registros: {total_registros:,}")
    logger.info(f"  Tama√±o total: {formatear_bytes(total_bytes)}")
    logger.info("")
    logger.info(f"  üìÑ Informe completo: {informe_path}")
    logger.info("")


# ==============================================================================
# PUNTO DE ENTRADA
# ==============================================================================

if __name__ == "__main__":
    main()
