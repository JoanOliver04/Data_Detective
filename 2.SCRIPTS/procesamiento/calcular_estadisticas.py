# -*- coding: utf-8 -*-
"""
==============================================================================
DATA DETECTIVE - VALENCIA
Fase 5.4: C√°lculo de Estad√≠sticas Agregadas
==============================================================================

Descripci√≥n:
    Calcula estad√≠sticas agregadas a partir de los datasets normalizados
    en las fases 5.1 (contaminaci√≥n) y 5.2 (meteorolog√≠a). Genera tres
    ficheros CSV con medias ponderadas por n√∫mero de registros v√°lidos.

Datos de entrada:
    1. 3.DATOS_LIMPIOS/contaminacion_normalizada.parquet
    2. 3.DATOS_LIMPIOS/meteorologia_limpio.csv

Datos de salida:
    1. 3.DATOS_LIMPIOS/estadisticas/contaminacion_media_anual_barrio.csv
       ‚Üí Medias anuales de contaminaci√≥n por barrio y variable
    2. 3.DATOS_LIMPIOS/estadisticas/precipitacion_media_mensual.csv
       ‚Üí Medias mensuales de precipitaci√≥n
    3. 3.DATOS_LIMPIOS/estadisticas/tendencias_historicas.csv
       ‚Üí Tendencias hist√≥ricas anuales (contaminaci√≥n + meteorolog√≠a)

Decisiones de dise√±o:
    - Solo se usan registros con calidad_dato == "ok" para contaminaci√≥n.
    - Solo se usan registros con calidad_dato == "ok" para meteorolog√≠a.
    - La media anual se calcula como sum(valor) / count(valor), que es
      equivalente a una media ponderada por n¬∫ de registros v√°lidos cuando
      las estaciones tienen distinta cobertura temporal.
    - Se incluye n_registros en cada fila para que el dashboard pueda
      mostrar la fiabilidad estad√≠stica de cada media.
    - El mapeo estacion_id ‚Üí barrio se define internamente (no depende
      de ficheros externos) para maximizar reproducibilidad.

Uso:
    python calcular_estadisticas.py

Commit sugerido:
    feat: add Phase 5.4 aggregated statistics with weighted annual means

Autor: Joan
Fecha: 2026
Proyecto: Data Detective Valencia
"""

import pandas as pd
import logging
import sys
from pathlib import Path
from typing import Optional, Tuple

# ==============================================================================
# CONFIGURACI√ìN
# ==============================================================================

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
DATOS_LIMPIOS_DIR = PROJECT_ROOT / "3.DATOS_LIMPIOS"
STATS_DIR = DATOS_LIMPIOS_DIR / "estadisticas"
LOG_DIR = PROJECT_ROOT / "logs"

# Ficheros de entrada
CONTAMINACION_FILE = DATOS_LIMPIOS_DIR / "contaminacion_normalizada.parquet"
METEOROLOGIA_FILE = DATOS_LIMPIOS_DIR / "meteorologia_limpio.csv"

# Ficheros de salida
OUT_CONTAM_ANUAL = STATS_DIR / "contaminacion_media_anual_barrio.csv"
OUT_PRECIP_MENSUAL = STATS_DIR / "precipitacion_media_mensual.csv"
OUT_TENDENCIAS = STATS_DIR / "tendencias_historicas.csv"

# ==============================================================================
# MAPEO ESTACI√ìN ‚Üí BARRIO
# ==============================================================================
# Basado en la ubicaci√≥n geogr√°fica real de cada estaci√≥n de medici√≥n.
# Fuentes: GVA (portal calidad del aire), AQICN (streaming_aqicn.py).
#
# ¬øPor qu√© un diccionario interno y no un CSV externo?
#   ‚Üí Son solo 6 estaciones fijas en Valencia ciudad. Un CSV externo
#     a√±adir√≠a complejidad innecesaria. Si en el futuro se a√±aden m√°s
#     estaciones, basta con ampliar este diccionario.

ESTACION_BARRIO_MAP = {
    # Estaciones GVA / EEA
    "46250001": "Quatre Carreres",       # Avd. Francia ‚Üí distrito Quatre Carreres
    "46250004": "Jes√∫s",                 # Pista de Silla antigua ‚Üí distrito Jes√∫s
    "46250030": "Jes√∫s",                 # Pista de Silla actual ‚Üí distrito Jes√∫s
    # Polit√®cnic (UPV) ‚Üí distrito Benimaclet
    "46250047": "Benimaclet",
    "46250050": "Patraix",               # Mol√≠ del Sol ‚Üí distrito Patraix
    "46250054": "Ciutat Vella",          # Conselleria Meteo ‚Üí distrito Ciutat Vella
}


# ==============================================================================
# CONFIGURACI√ìN DE LOGGING
# ==============================================================================

def setup_logging() -> logging.Logger:
    """
    Configura logging dual (archivo + consola).
    Mismo patr√≥n que normalizar_contaminacion.py (Fase 5.1).
    """
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    log_file = LOG_DIR / "calcular_estadisticas.log"
    log_format = "%(asctime)s - %(levelname)s - %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"

    logger = logging.getLogger("Calcular_Estadisticas")
    logger.setLevel(logging.DEBUG)
    logger.handlers.clear()

    # Archivo (todo)
    fh = logging.FileHandler(log_file, encoding="utf-8", mode="a")
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter(log_format, date_format))

    # Consola (INFO+)
    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter(log_format, date_format))

    logger.addHandler(fh)
    logger.addHandler(ch)

    return logger


# ==============================================================================
# CARGA DE DATOS
# ==============================================================================

def cargar_contaminacion(logger: logging.Logger) -> Optional[pd.DataFrame]:
    """
    Carga el Parquet normalizado de contaminaci√≥n (Fase 5.1).

    Esquema esperado:
        fecha_utc, estacion_id, estacion_nombre, fuente,
        variable, valor, unidad, calidad_dato

    Returns:
        DataFrame o None si el fichero no existe / est√° vac√≠o.
    """
    if not CONTAMINACION_FILE.exists():
        logger.error(f"No se encuentra: {CONTAMINACION_FILE}")
        logger.error("Ejecuta primero normalizar_contaminacion.py (Fase 5.1)")
        return None

    logger.info(f"Cargando contaminaci√≥n: {CONTAMINACION_FILE.name}")

    try:
        df = pd.read_parquet(CONTAMINACION_FILE)
    except Exception as e:
        logger.error(f"Error leyendo Parquet: {e}")
        return None

    if df.empty:
        logger.warning("Fichero de contaminaci√≥n vac√≠o")
        return None

    logger.info(f"  ‚Üí {len(df):,} registros cargados")
    logger.info(f"  ‚Üí Columnas: {list(df.columns)}")
    logger.info(
        f"  ‚Üí Rango: {df['fecha_utc'].min()} ‚Üí {df['fecha_utc'].max()}")

    return df


def cargar_meteorologia(logger: logging.Logger) -> Optional[pd.DataFrame]:
    """
    Carga el CSV normalizado de meteorolog√≠a (Fase 5.2).

    Esquema esperado:
        fecha, hora, precipitacion_mm, temp_c, humedad_pct,
        fuente, calidad_dato

    Returns:
        DataFrame o None si el fichero no existe / est√° vac√≠o.
    """
    if not METEOROLOGIA_FILE.exists():
        logger.error(f"No se encuentra: {METEOROLOGIA_FILE}")
        logger.error("Ejecuta primero limpiar_meteorologia.py (Fase 5.2)")
        return None

    logger.info(f"Cargando meteorolog√≠a: {METEOROLOGIA_FILE.name}")

    try:
        # Cargar primero sin parsear fechas para inspeccionar
        df = pd.read_csv(METEOROLOGIA_FILE)

        # Intentar convertir fecha a datetime de forma robusta
        # errors='coerce' convertir√° valores inv√°lidos a NaT (Not a Time)
        df["fecha"] = pd.to_datetime(df["fecha"], errors='coerce', utc=True)

        # Verificar si hay fechas inv√°lidas
        fechas_invalidas = df["fecha"].isna().sum()
        if fechas_invalidas > 0:
            logger.warning(
                f"  {fechas_invalidas:,} fechas inv√°lidas encontradas y convertidas a NaT")
            # Opcional: mostrar algunos ejemplos de fechas inv√°lidas
            muestra_invalidas = df[df["fecha"].isna()].head(3)
            if not muestra_invalidas.empty:
                logger.debug(
                    f"  Ejemplos de fechas inv√°lidas: {muestra_invalidas.iloc[:, 0].tolist()}")

    except Exception as e:
        logger.error(f"Error leyendo CSV: {e}")
        return None

    if df.empty:
        logger.warning("Fichero de meteorolog√≠a vac√≠o")
        return None

    logger.info(f"  ‚Üí {len(df):,} registros cargados")
    logger.info(f"  ‚Üí Columnas: {list(df.columns)}")
    logger.info(f"  ‚Üí Rango: {df['fecha'].min()} ‚Üí {df['fecha'].max()}")

    return df

# ==============================================================================
# TAREA 1: MEDIAS ANUALES DE CONTAMINACI√ìN POR BARRIO
# ==============================================================================


def calcular_contaminacion_anual_barrio(
    df: pd.DataFrame,
    logger: logging.Logger
) -> Optional[pd.DataFrame]:
    """
    Calcula medias anuales de contaminaci√≥n por barrio y variable.

    Proceso:
        1. Filtra solo registros con calidad_dato == "ok"
        2. Extrae el a√±o desde fecha_utc
        3. Mapea estacion_id ‚Üí barrio usando ESTACION_BARRIO_MAP
        4. Agrupa por (a√±o, barrio, variable)
        5. Calcula: media_anual = sum(valor) / count(valor)
        6. A√±ade n_registros para evaluar fiabilidad

    Args:
        df: DataFrame de contaminaci√≥n normalizada
        logger: Logger configurado

    Returns:
        DataFrame con columnas:
            [a√±o, barrio, variable, media_anual, n_registros, unidad]
    """
    logger.info("‚îÄ" * 40)
    logger.info("TAREA 1: Medias anuales de contaminaci√≥n por barrio")
    logger.info("‚îÄ" * 40)

    # Paso 1: Filtrar solo datos v√°lidos
    df_ok = df[df["calidad_dato"] == "ok"].copy()
    descartados = len(df) - len(df_ok)
    logger.info(f"  Registros v√°lidos (ok): {len(df_ok):,} / {len(df):,}")
    if descartados > 0:
        logger.info(f"  Descartados (invalid/missing): {descartados:,}")

    if df_ok.empty:
        logger.warning("  Sin datos v√°lidos para calcular estad√≠sticas")
        return None

    # Paso 2: Extraer a√±o
    # fecha_utc puede ser tz-aware (UTC) ‚Üí extraemos .dt.year directamente
    df_ok["a√±o"] = df_ok["fecha_utc"].dt.year

    # Paso 3: Mapear estacion_id ‚Üí barrio
    df_ok["barrio"] = df_ok["estacion_id"].map(ESTACION_BARRIO_MAP)

    # Registrar estaciones sin mapeo (por si hay estaciones nuevas)
    sin_barrio = df_ok[df_ok["barrio"].isna()]["estacion_id"].unique()
    if len(sin_barrio) > 0:
        logger.warning(
            f"  Estaciones sin mapeo a barrio: {list(sin_barrio)}. "
            f"Se excluyen del c√°lculo. Actualiza ESTACION_BARRIO_MAP si es necesario."
        )
        df_ok = df_ok.dropna(subset=["barrio"])

    if df_ok.empty:
        logger.warning("  Sin datos tras mapeo de barrios")
        return None

    # Paso 4-5: Agrupar y calcular media
    # media_anual = sum(valor) / count(valor) ‚Üí equivalente a .mean()
    # n_registros = count(valor) ‚Üí para ponderar fiabilidad
    df_stats = (
        df_ok
        .groupby(["a√±o", "barrio", "variable"], as_index=False)
        .agg(
            media_anual=("valor", "mean"),
            n_registros=("valor", "count"),
        )
    )

    # A√±adir unidad (siempre ¬µg/m¬≥ para contaminaci√≥n)
    df_stats["unidad"] = "¬µg/m¬≥"

    # Redondear media a 2 decimales
    df_stats["media_anual"] = df_stats["media_anual"].round(2)

    # Ordenar para legibilidad
    df_stats = df_stats.sort_values(
        ["a√±o", "barrio", "variable"]
    ).reset_index(drop=True)

    logger.info(f"  Resultado: {len(df_stats):,} filas")
    logger.info(
        f"  A√±os cubiertos: {df_stats['a√±o'].min()} ‚Üí {df_stats['a√±o'].max()}")
    logger.info(f"  Barrios: {sorted(df_stats['barrio'].unique())}")
    logger.info(f"  Variables: {sorted(df_stats['variable'].unique())}")

    return df_stats


# ==============================================================================
# TAREA 2: MEDIAS MENSUALES DE PRECIPITACI√ìN
# ==============================================================================

def calcular_precipitacion_mensual(
    df: pd.DataFrame,
    logger: logging.Logger
) -> Optional[pd.DataFrame]:
    """
    Calcula medias mensuales de precipitaci√≥n.

    Proceso:
        1. Filtra solo registros con calidad_dato == "ok"
        2. Descarta filas donde precipitacion_mm es NaN o fecha es NaT
        3. Extrae a√±o y mes desde fecha
        4. Agrupa por (a√±o, mes)
        5. Calcula media mensual + n_registros
    """
    logger.info("")
    logger.info("‚îÄ" * 40)
    logger.info("TAREA 2: Medias mensuales de precipitaci√≥n")
    logger.info("‚îÄ" * 40)

    # Paso 1: Filtrar datos v√°lidos
    df_ok = df[df["calidad_dato"] == "ok"].copy()
    logger.info(f"  Registros v√°lidos (ok): {len(df_ok):,} / {len(df):,}")

    # Paso 2: Descartar NaN en precipitaci√≥n Y fechas inv√°lidas (NaT)
    df_precip = df_ok.dropna(subset=["precipitacion_mm", "fecha"]).copy()
    logger.info(f"  Con precipitaci√≥n y fecha v√°lidas: {len(df_precip):,}")

    if df_precip.empty:
        logger.warning("  Sin datos de precipitaci√≥n v√°lidos")
        return None

    # Verificar que fecha es realmente datetime
    if not pd.api.types.is_datetime64_any_dtype(df_precip["fecha"]):
        logger.error(
            "  La columna 'fecha' no es de tipo datetime despu√©s de la limpieza")
        return None

    # Paso 3: Extraer a√±o y mes
    df_precip["a√±o"] = df_precip["fecha"].dt.year
    df_precip["mes"] = df_precip["fecha"].dt.month

    # Paso 4-5: Agrupar y calcular
    df_stats = (
        df_precip
        .groupby(["a√±o", "mes"], as_index=False)
        .agg(
            precipitacion_media_mm=("precipitacion_mm", "mean"),
            n_registros=("precipitacion_mm", "count"),
        )
    )

    # Redondear
    df_stats["precipitacion_media_mm"] = df_stats["precipitacion_media_mm"].round(
        2)

    # Ordenar
    df_stats = df_stats.sort_values(["a√±o", "mes"]).reset_index(drop=True)

    logger.info(f"  Resultado: {len(df_stats):,} filas (meses √∫nicos)")
    if not df_stats.empty:
        logger.info(f"  Rango: {df_stats['a√±o'].min()}/{df_stats['mes'].min():02d} "
                    f"‚Üí {df_stats['a√±o'].max()}/{df_stats['mes'].max():02d}")

    return df_stats

# ==============================================================================
# TAREA 3: TENDENCIAS HIST√ìRICAS (1963-2026)
# ==============================================================================


def calcular_tendencias_historicas(
    df_contam: Optional[pd.DataFrame],
    df_meteo: Optional[pd.DataFrame],
    logger: logging.Logger
) -> Optional[pd.DataFrame]:
    """
    Calcula tendencias hist√≥ricas anuales combinando contaminaci√≥n y meteorolog√≠a.

    Contaminaci√≥n: media anual global ponderada por variable (sin desglose barrio).
    Meteorolog√≠a: media anual de temp_c, precipitacion_mm y humedad_pct.

    Ambas fuentes se unen por a√±o para tener una vista integrada de la
    evoluci√≥n temporal de Valencia.

    Args:
        df_contam: DataFrame de contaminaci√≥n normalizada (o None)
        df_meteo: DataFrame de meteorolog√≠a normalizada (o None)
        logger: Logger configurado

    Returns:
        DataFrame con una fila por a√±o y columnas para cada variable.
    """
    logger.info("")
    logger.info("‚îÄ" * 40)
    logger.info("TAREA 3: Tendencias hist√≥ricas (1963-2026)")
    logger.info("‚îÄ" * 40)

    frames = []

    # --- 3A: Contaminaci√≥n ‚Üí media anual global por variable ---
    if df_contam is not None:
        logger.info("  3A: Procesando contaminaci√≥n...")

        df_ok = df_contam[df_contam["calidad_dato"] == "ok"].copy()
        df_ok["a√±o"] = df_ok["fecha_utc"].dt.year

        # Pivotar: una columna por variable (NO2, O3, PM10, etc.)
        # Para cada (a√±o, variable): media de todos los valores v√°lidos
        contam_anual = (
            df_ok
            .groupby(["a√±o", "variable"], as_index=False)
            .agg(
                media=("valor", "mean"),
                n=("valor", "count"),
            )
        )

        # Pivotar a formato ancho: a√±o | NO2_media | NO2_n | O3_media | ...
        pivot_media = contam_anual.pivot(
            index="a√±o", columns="variable", values="media"
        )
        pivot_n = contam_anual.pivot(
            index="a√±o", columns="variable", values="n"
        )

        # Renombrar columnas: NO2 ‚Üí NO2_ugm3, para claridad
        pivot_media.columns = [f"{col}_ugm3" for col in pivot_media.columns]
        pivot_n.columns = [f"{col}_n_registros" for col in pivot_n.columns]

        # Unir media + n_registros
        contam_wide = pd.concat([pivot_media, pivot_n], axis=1)
        contam_wide = contam_wide.round(2)

        frames.append(contam_wide)
        logger.info(f"  ‚Üí Contaminaci√≥n: {len(contam_wide)} a√±os")

    # --- 3B: Meteorolog√≠a ‚Üí media anual por variable ---
    if df_meteo is not None:
        logger.info("  3B: Procesando meteorolog√≠a...")

        df_ok = df_meteo[df_meteo["calidad_dato"] == "ok"].copy()
        df_ok["a√±o"] = df_ok["fecha"].dt.year

        # Calcular medias anuales para cada variable meteorol√≥gica
        meteo_stats = []

        for col, nombre_salida in [
            ("temp_c", "temp_media_c"),
            ("precipitacion_mm", "precipitacion_media_mm"),
            ("humedad_pct", "humedad_media_pct"),
        ]:
            if col not in df_ok.columns:
                continue

            serie = df_ok.dropna(subset=[col]).groupby("a√±o")[col]
            media = serie.mean().rename(nombre_salida)
            n = serie.count().rename(
                f"{nombre_salida.replace('media_', '').replace('_media', '')}_n_registros")

            meteo_stats.extend([media, n])

        if meteo_stats:
            meteo_wide = pd.concat(meteo_stats, axis=1).round(2)
            frames.append(meteo_wide)
            logger.info(f"  ‚Üí Meteorolog√≠a: {len(meteo_wide)} a√±os")

    # --- Unir ambas fuentes por a√±o ---
    if not frames:
        logger.warning("  Sin datos para tendencias hist√≥ricas")
        return None

    # outer join para no perder a√±os que solo tienen una fuente
    df_tendencias = pd.concat(frames, axis=1)
    df_tendencias.index.name = "a√±o"
    df_tendencias = df_tendencias.reset_index()

    # Ordenar por a√±o
    df_tendencias = df_tendencias.sort_values("a√±o").reset_index(drop=True)

    logger.info(f"  Resultado final: {len(df_tendencias)} a√±os")
    logger.info(
        f"  Rango: {df_tendencias['a√±o'].min()} ‚Üí {df_tendencias['a√±o'].max()}")
    logger.info(f"  Columnas: {list(df_tendencias.columns)}")

    return df_tendencias


# ==============================================================================
# GUARDADO
# ==============================================================================

def guardar_csv(
    df: pd.DataFrame,
    path: Path,
    logger: logging.Logger,
    descripcion: str
) -> bool:
    """
    Guarda un DataFrame como CSV con encoding UTF-8.

    Args:
        df: DataFrame a guardar
        path: Ruta de destino
        logger: Logger
        descripcion: Descripci√≥n para el log (ej: "contaminaci√≥n anual")

    Returns:
        True si se guard√≥ correctamente, False si hubo error.
    """
    try:
        df.to_csv(path, index=False, encoding="utf-8-sig")
        logger.info(f"  ‚úî Guardado [{descripcion}]: {path}")
        logger.info(f"    ‚Üí {len(df):,} filas, {len(df.columns)} columnas")
        return True
    except Exception as e:
        logger.error(f"  ‚úò Error guardando [{descripcion}]: {e}")
        return False


# ==============================================================================
# RESUMEN FINAL
# ==============================================================================

def imprimir_resumen(
    df_contam_barrio: Optional[pd.DataFrame],
    df_precip_mensual: Optional[pd.DataFrame],
    df_tendencias: Optional[pd.DataFrame],
    logger: logging.Logger
) -> None:
    """
    Imprime un resumen ejecutivo de todas las estad√≠sticas calculadas.
    """
    logger.info("")
    logger.info("=" * 70)
    logger.info("RESUMEN FINAL - ESTAD√çSTICAS AGREGADAS (Fase 5.4)")
    logger.info("=" * 70)

    # Tarea 1: Contaminaci√≥n anual por barrio
    if df_contam_barrio is not None:
        logger.info("")
        logger.info("  üìä Contaminaci√≥n media anual por barrio:")
        logger.info(f"     Filas: {len(df_contam_barrio):,}")
        logger.info(f"     A√±os: {df_contam_barrio['a√±o'].min()} ‚Üí "
                    f"{df_contam_barrio['a√±o'].max()}")
        logger.info(
            f"     Barrios: {sorted(df_contam_barrio['barrio'].unique())}")
        logger.info(
            f"     Variables: {sorted(df_contam_barrio['variable'].unique())}")

        # Top 3 combinaciones con m√°s registros
        top = df_contam_barrio.nlargest(3, "n_registros")
        for _, row in top.iterrows():
            logger.info(
                f"     Top: {row['barrio']}/{row['variable']} "
                f"({row['a√±o']}): {row['media_anual']:.1f} ¬µg/m¬≥ "
                f"(n={row['n_registros']:,})"
            )
    else:
        logger.warning("  üìä Contaminaci√≥n anual por barrio: NO GENERADO")

    # Tarea 2: Precipitaci√≥n mensual
    if df_precip_mensual is not None:
        logger.info("")
        logger.info("  üåßÔ∏è  Precipitaci√≥n media mensual:")
        logger.info(f"     Filas: {len(df_precip_mensual):,}")
        logger.info(f"     Rango: {df_precip_mensual['a√±o'].min()}/{df_precip_mensual['mes'].min():02d} ‚Üí "
                    f"{df_precip_mensual['a√±o'].max()}/{df_precip_mensual['mes'].max():02d}")

        # Mes m√°s lluvioso global
        mes_max = df_precip_mensual.loc[
            df_precip_mensual["precipitacion_media_mm"].idxmax()
        ]
        logger.info(
            f"     M√°s lluvioso: {int(mes_max['a√±o'])}/{int(mes_max['mes']):02d} "
            f"‚Üí {mes_max['precipitacion_media_mm']:.1f} mm/d√≠a "
            f"(n={int(mes_max['n_registros']):,})"
        )
    else:
        logger.warning("  üåßÔ∏è  Precipitaci√≥n mensual: NO GENERADO")

    # Tarea 3: Tendencias hist√≥ricas
    if df_tendencias is not None:
        logger.info("")
        logger.info("  üìà Tendencias hist√≥ricas:")
        logger.info(f"     Filas: {len(df_tendencias):,}")
        logger.info(f"     Rango: {df_tendencias['a√±o'].min()} ‚Üí "
                    f"{df_tendencias['a√±o'].max()}")
        logger.info(f"     Columnas: {len(df_tendencias.columns)}")
    else:
        logger.warning("  üìà Tendencias hist√≥ricas: NO GENERADO")

    logger.info("")
    logger.info("=" * 70)


# ==============================================================================
# FUNCI√ìN PRINCIPAL
# ==============================================================================

def main():
    """
    Orquesta el c√°lculo completo de estad√≠sticas agregadas.

    Flujo:
        1. Cargar datos normalizados
        2. Calcular medias anuales de contaminaci√≥n por barrio
        3. Calcular medias mensuales de precipitaci√≥n
        4. Calcular tendencias hist√≥ricas
        5. Guardar todos los CSV
        6. Imprimir resumen
    """
    logger = setup_logging()
    logger.info("=" * 70)
    logger.info("DATA DETECTIVE - Fase 5.4: Estad√≠sticas Agregadas")
    logger.info("=" * 70)

    # Crear carpeta de salida
    STATS_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"Carpeta de salida: {STATS_DIR}")

    # ------------------------------------------------------------------
    # PASO 1: Cargar datos
    # ------------------------------------------------------------------
    logger.info("")
    logger.info("‚îÄ" * 40)
    logger.info("CARGA DE DATOS")
    logger.info("‚îÄ" * 40)

    df_contam = cargar_contaminacion(logger)
    df_meteo = cargar_meteorologia(logger)

    if df_contam is None and df_meteo is None:
        logger.error("No hay datos de ninguna fuente. Abortando.")
        print("\n‚ùå ERROR: No se encontraron datos. Revisa las fases 5.1 y 5.2.")
        return

    # ------------------------------------------------------------------
    # PASO 2: Tarea 1 - Contaminaci√≥n anual por barrio
    # ------------------------------------------------------------------
    df_contam_barrio = None
    if df_contam is not None:
        logger.info("")
        df_contam_barrio = calcular_contaminacion_anual_barrio(
            df_contam, logger)

    # ------------------------------------------------------------------
    # PASO 3: Tarea 2 - Precipitaci√≥n mensual
    # ------------------------------------------------------------------
    df_precip_mensual = None
    if df_meteo is not None:
        df_precip_mensual = calcular_precipitacion_mensual(df_meteo, logger)

    # ------------------------------------------------------------------
    # PASO 4: Tarea 3 - Tendencias hist√≥ricas
    # ------------------------------------------------------------------
    df_tendencias = calcular_tendencias_historicas(df_contam, df_meteo, logger)

    # ------------------------------------------------------------------
    # PASO 5: Guardar resultados
    # ------------------------------------------------------------------
    logger.info("")
    logger.info("‚îÄ" * 40)
    logger.info("GUARDADO DE RESULTADOS")
    logger.info("‚îÄ" * 40)

    guardados = 0

    if df_contam_barrio is not None:
        if guardar_csv(df_contam_barrio, OUT_CONTAM_ANUAL, logger,
                       "Contaminaci√≥n anual por barrio"):
            guardados += 1

    if df_precip_mensual is not None:
        if guardar_csv(df_precip_mensual, OUT_PRECIP_MENSUAL, logger,
                       "Precipitaci√≥n media mensual"):
            guardados += 1

    if df_tendencias is not None:
        if guardar_csv(df_tendencias, OUT_TENDENCIAS, logger,
                       "Tendencias hist√≥ricas"):
            guardados += 1

    # ------------------------------------------------------------------
    # PASO 6: Resumen
    # ------------------------------------------------------------------
    imprimir_resumen(df_contam_barrio, df_precip_mensual,
                     df_tendencias, logger)

    # Mensaje final consola
    print(f"\n‚úÖ ESTAD√çSTICAS COMPLETADAS: {guardados}/3 ficheros generados")
    if df_contam_barrio is not None:
        print(f"   ‚Üí {OUT_CONTAM_ANUAL}")
    if df_precip_mensual is not None:
        print(f"   ‚Üí {OUT_PRECIP_MENSUAL}")
    if df_tendencias is not None:
        print(f"   ‚Üí {OUT_TENDENCIAS}")

    print(f"\nCommit sugerido:")
    print(f"   git add 2.SCRIPTS/procesamiento/calcular_estadisticas.py")
    print(f"   git add 3.DATOS_LIMPIOS/estadisticas/")
    print(f'   git commit -m "feat: add Phase 5.4 aggregated statistics with weighted annual means"')


# ==============================================================================
# PUNTO DE ENTRADA
# ==============================================================================

if __name__ == "__main__":
    main()
